// test_sentiment.js

import { pipeline } from "@xenova/transformers";

async function main() {
  console.log("üîç Starting minimal sentiment test‚Ä¶");

  // 1Ô∏è‚É£ Load the sentiment-analysis pipeline from Xenova
  let sentimenter;
  try {
    sentimenter = await pipeline(
      "sentiment-analysis",
      "Xenova/twitter-roberta-base-sentiment-latest"
    );
    console.log("‚úÖ Pipeline loaded! Type:", typeof sentimenter);
  } catch (err) {
    console.error("‚ùå Failed to load pipeline:", err);
    return;
  }

  // 2Ô∏è‚É£ Ensure the pipeline‚Äôs tokenizer has .encode() and .decode()
  if (
    !sentimenter.tokenizer ||
    typeof sentimenter.tokenizer.encode !== "function" ||
    typeof sentimenter.tokenizer.decode !== "function"
  ) {
    console.error(
      "‚ùå The loaded pipeline is missing .tokenizer.encode() or .tokenizer.decode()!"
    );
    return;
  }
  console.log("üõ†Ô∏è  Tokenizer is ready (encode & decode methods found).");

  // Helper function to truncate a string to ‚â§ maxTokens:
  async function truncateToMaxTokens(text, maxTokens = 500) {
    // ‚ú® Encode the input text ‚Üí this returns a number[] of token IDs.
    const ids = sentimenter.tokenizer.encode(text);
    const tokenCount = ids.length;

    if (tokenCount <= maxTokens) {
      // No need to truncate
      return text;
    }

    console.warn(
      `‚ö†Ô∏è Input has ${tokenCount} tokens (too long). Truncating to ${maxTokens} tokens‚Ä¶`
    );
    // Slice the first maxTokens IDs
    const truncatedIds = ids.slice(0, maxTokens);
    // Decode back into a string
    const truncatedString = sentimenter.tokenizer.decode(truncatedIds);
    return truncatedString;
  }

  // 3Ô∏è‚É£ Test with a normal‚Äêlength string
  const shortText =
    "I love sunny days and reading a good book by the beach!";
  console.log("\nüìù Test 1: Short text:", shortText);
  try {
    // Truncate (will no-op because it‚Äôs already short)
    const text1 = await truncateToMaxTokens(shortText);
    // Run inference
    const result1 = await sentimenter(text1);
    console.log("üëâ Sentiment result:", result1);
    // Example output: [ { label: "POSITIVE", score: 0.998 } ]
  } catch (err) {
    console.error("‚ùå Error during inference (short text):", err);
  }

  // 4Ô∏è‚É£ Test with a super‚Äêlong string (‚âà3000 ‚Äúhello‚Äù words)
  const manyWords = Array(3000).fill("hello").join(" ");
  console.log("\nüìù Test 2: Very long text (~3000 words)‚Ä¶");
  try {
    // Truncate down to ‚â§500 tokens
    const truncatedLong = await truncateToMaxTokens(manyWords);
    // After truncation, count tokens again:
    const afterIds = sentimenter.tokenizer.encode(truncatedLong);
    console.log("‚Üí After truncation: token count =", afterIds.length);

    // Run inference on the truncated string
    const result2 = await sentimenter(truncatedLong);
    console.log("üëâ Sentiment result (after truncate):", result2);
    console.log(
      "üéâ No expand‚Äêshape crash because we truncated under 512 tokens!"
    );
  } catch (err) {
    console.error("‚ùå Error during inference (long text):", err);
  }

  console.log("\nüèÅ Test complete.");
}

main();
